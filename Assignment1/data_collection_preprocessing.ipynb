{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e944018e",
   "metadata": {},
   "source": [
    "Assignment1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a774ed0",
   "metadata": {},
   "source": [
    "Dataset Overview\n",
    "Data Source\n",
    "Dataset name: Wikipedia English Dump\n",
    "\t•Provider: Hugging Face Datasets\n",
    "\t•Configuration: wikipedia / 20220301.en\n",
    "\t•Access mode: Streaming (streaming=True)\n",
    "\n",
    "The dataset is accessed in streaming mode to avoid downloading the full corpus locally and to reduce memory usage during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beac1ba",
   "metadata": {},
   "source": [
    "Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab72813f-61d0-4630-ba4c-ef25ff31f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.9 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 19:09:58) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b132b",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824e19e7-605b-43a9-979e-09bf1f399ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in e:\\anaconda\\conda\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda\\conda\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in e:\\anaconda\\conda\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in e:\\anaconda\\conda\\lib\\site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in e:\\anaconda\\conda\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\conda\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\anaconda\\conda\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\anaconda\\conda\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: shellingham in e:\\anaconda\\conda\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in e:\\anaconda\\conda\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda\\conda\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\anaconda\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\anaconda\\conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\conda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda\\conda\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\anaconda\\conda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\conda\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda\\conda\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\anaconda\\conda\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.2.1)\n",
      "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "Downloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 7.6/12.0 MB 40.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 39.8 MB/s  0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 23.1 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 41.6 MB/s  0:00:00\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: xxhash, safetensors, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "\n",
      "   ----------- ---------------------------- 2/7 [multiprocess]\n",
      "   ----------- ---------------------------- 2/7 [multiprocess]\n",
      "   ----------- ---------------------------- 2/7 [multiprocess]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ----------------- ---------------------- 3/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [tokenizers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------- ----------- 5/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------- ----- 6/7 [datasets]\n",
      "   ---------------------------------------- 7/7 [datasets]\n",
      "\n",
      "Successfully installed datasets-4.5.0 huggingface-hub-0.36.0 multiprocess-0.70.18 safetensors-0.7.0 tokenizers-0.22.2 transformers-4.57.6 xxhash-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7c8db",
   "metadata": {},
   "source": [
    "Load dataset with streaming\n",
    "\n",
    "This uses Hugging Face Datasets in streaming mode so the full corpus is not downloaded into local disk/RAM.\n",
    "\n",
    "Inspect one sample\n",
    "\n",
    "Verify the sample structure and confirm which field contains the raw article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a49330e-0429-436b-b632-1295380d6a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606fc8d64f7e406ebc1e6e02642410ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['id', 'url', 'title', 'text'])\n",
      "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.en\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "sample = next(iter(ds))\n",
    "print(type(sample))\n",
    "print(sample.keys())\n",
    "print(sample[\"text\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e45845",
   "metadata": {},
   "source": [
    "Define clean_text\n",
    "\n",
    "Cleaning strategy implemented:\n",
    "\t•handle None\n",
    "\t•collapse multiple whitespaces into single space\n",
    "\t•strip()\n",
    "\t•lowercase\n",
    "\t•filter very short documents by word count threshold (min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de77b565-71a8-456a-abbe-6dfa85559dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text, min_words=50):\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    if len(text.split()) < min_words:\n",
    "        return None\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6c4d7",
   "metadata": {},
   "source": [
    "Quick sanity check for cleaning\n",
    "\n",
    "This iterates over the streaming dataset and prints the first 3 cleaned outputs to confirm cleaning/filtering works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b2cd51-1164-4bab-a783-bf87030eb115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. anarchism advocates for the replacement of the state \n",
      "albedo (; ) is the fraction of sunlight that is diffusely reflected by a body. it is measured on a scale from 0 (corresponding to a black body that absorbs all incident radiation) to 1 (corresponding to a body that reflects all incident radiation). surface albedo is defined as the ratio of radiosity\n",
      "a, or a, is the first letter and the first vowel of the latin alphabet, used in the modern english alphabet, the alphabets of other western european languages and others worldwide. its name in english is a (pronounced ), plural aes. it is similar in shape to the ancient greek letter alpha, from whic\n"
     ]
    }
   ],
   "source": [
    "cleaned_count = 0\n",
    "\n",
    "for sample in ds:\n",
    "    cleaned = clean_text(sample[\"text\"])\n",
    "    if cleaned:\n",
    "        print(cleaned[:300])\n",
    "        cleaned_count += 1\n",
    "    if cleaned_count >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da4283",
   "metadata": {},
   "source": [
    "Define is_duplicate using MD5 hashes\n",
    "\n",
    "This maintains a seen_hashes set and marks a text as duplicate if its MD5 hash already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b7a07c5-8a92-4c35-84cb-419f46cb45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def is_duplicate(text, seen_hashes):\n",
    "    h = hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "    if h in seen_hashes:\n",
    "        return True\n",
    "    seen_hashes.add(h)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee2215",
   "metadata": {},
   "source": [
    "Sanity check for dedup + cleaning\n",
    "\n",
    "Stream through dataset → clean → dedup → print first 3 unique cleaned samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffbe4fa0-380e-4f0e-953a-ba694c2441de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typi\n",
      "albedo (; ) is the fraction of sunlight that is diffusely reflected by a body. it is measured on a scale from 0 (corresponding to a black body that absorbs all incident radiation) to 1 (corresponding \n",
      "a, or a, is the first letter and the first vowel of the latin alphabet, used in the modern english alphabet, the alphabets of other western european languages and others worldwide. its name in english\n"
     ]
    }
   ],
   "source": [
    "seen_hashes = set()\n",
    "kept = 0\n",
    "\n",
    "for sample in ds:\n",
    "    cleaned = clean_text(sample[\"text\"])\n",
    "    if not cleaned:\n",
    "        continue\n",
    "\n",
    "    if is_duplicate(cleaned, seen_hashes):\n",
    "        continue\n",
    "\n",
    "    print(cleaned[:200])\n",
    "    kept += 1\n",
    "\n",
    "    if kept >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a05bc",
   "metadata": {},
   "source": [
    "Initialize tokenizer\n",
    "\n",
    "Load GPT-2 tokenizer and set padding token to EOS (common quick fix since GPT-2 has no pad token by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "089219a1-ed86-43b3-b91b-cff86a06ce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639cfa22acec4b2c8deaa9004a30b35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cc1b8737644fcca5b4efa1469279da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f230d93aeb94776b7f3495127a0fbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d8514d93248d5835d1e25ee1ac7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c92dff3b7b42c392f7f3766969ecd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c543ee",
   "metadata": {},
   "source": [
    "Define chunk_token_ids\n",
    "\n",
    "This yields non-overlapping blocks of exactly block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d93bbf3a-c67b-4a3b-981b-3fed87293e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_token_ids(token_ids, block_size=512):\n",
    "    for i in range(0, len(token_ids) - block_size + 1, block_size):\n",
    "        yield token_ids[i:i + block_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ebbb2",
   "metadata": {},
   "source": [
    "Define iter_token_blocks\n",
    "\n",
    "Pipeline implemented inside one iterator:\n",
    "\t1.clean_text(sample[\"text\"])\n",
    "\t2.dedup by MD5 (local seen_hashes)\n",
    "\t3.tokenize (add_special_tokens=False)\n",
    "\t4.chunk into fixed-size blocks\n",
    "\t5.stop after max_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df351785-b2ed-4db3-9239-585cf91b63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_token_blocks(ds, tokenizer, block_size=512, max_blocks=10):\n",
    "    import hashlib\n",
    "\n",
    "    seen_hashes = set()\n",
    "    produced = 0\n",
    "\n",
    "    for sample in ds:\n",
    "        cleaned = clean_text(sample[\"text\"])\n",
    "        if not cleaned:\n",
    "            continue\n",
    "\n",
    "        h = hashlib.md5(cleaned.encode(\"utf-8\")).hexdigest()\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "\n",
    "        ids = tokenizer(cleaned, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        # chunking\n",
    "        for block in chunk_token_ids(ids, block_size=block_size):\n",
    "            yield block\n",
    "            produced += 1\n",
    "            if produced >= max_blocks:\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad62883",
   "metadata": {},
   "source": [
    "Smoke test the iterator + decode preview\n",
    "\n",
    "Generate a few blocks (block_size=128, max_blocks=3) and print:\n",
    "\t•block length\n",
    "\t•first token ids\n",
    "\t•decoded preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5a8480-ccaf-47ff-aab6-6cec7454d413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8524 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 length: 128\n",
      "First 30 token ids: [272, 998, 1042, 318, 257, 1964, 8876, 290, 3356, 326, 318, 17988, 286, 477, 655, 6637, 329, 4934, 290, 12932, 284, 35531, 262, 6712, 340, 3667, 5529, 13114, 32000, 290]\n",
      "Decoded preview: anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. as a historically left-wing movement, this reading of anarchism is placed on the farthest left of the political\n",
      "------------------------------------------------------------\n",
      "Block 1 length: 128\n",
      "First 30 token ids: [3812, 4934, 635, 8278, 13, 3584, 20675, 286, 26177, 4213, 389, 1043, 477, 3690, 2106, 11, 3660, 41661, 9349, 422, 262, 35957, 13, 1141, 262, 6846, 2063, 286, 262, 678]\n",
      "Decoded preview:  toward authority also rose. although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the enlightenment. during the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. various anarchist schools of thought formed during this period. anarchists have taken part in several revolutions\n",
      "------------------------------------------------------------\n",
      "Block 2 length: 128\n",
      "First 30 token ids: [11, 262, 26177, 3356, 468, 587, 33316, 298, 1752, 517, 11, 3957, 287, 11533, 290, 4588, 1626, 3098, 12, 49970, 11, 3098, 12, 5767, 290, 3098, 12, 20541, 5612, 8650]\n",
      "Decoded preview: , the anarchist movement has been resurgent once more, growing in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements. anarchists employ diverse approaches, which may be generally divided into revolutionary and evolutionary strategies; there is significant overlap between the two. evolutionary methods try to simulate what an anarchist society might be like, but revolutionary tactics, which have historically taken a violent turn,\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "blocks = list(iter_token_blocks(ds, tokenizer, block_size=128, max_blocks=3))\n",
    "\n",
    "for i, b in enumerate(blocks):\n",
    "    print(f\"Block {i} length:\", len(b))\n",
    "    print(\"First 30 token ids:\", b[:30])\n",
    "    print(\"Decoded preview:\", tokenizer.decode(b[:80]))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b48f9",
   "metadata": {},
   "source": [
    "Install/import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a07c6e-dcdd-461f-bff1-d7376554cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\conda\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\anaconda\\conda\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in e:\\anaconda\\conda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in e:\\anaconda\\conda\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\conda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in e:\\anaconda\\conda\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\conda\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda\\conda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda\\conda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 5.2/110.9 MB 30.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 15.2/110.9 MB 40.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 25.2/110.9 MB 42.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 35.1/110.9 MB 43.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 44.8/110.9 MB 44.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 54.8/110.9 MB 44.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 64.7/110.9 MB 45.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 74.7/110.9 MB 45.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 84.4/110.9 MB 45.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 94.1/110.9 MB 45.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.8/110.9 MB 45.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 45.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.9/110.9 MB 44.0 MB/s  0:00:02\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4696ea8",
   "metadata": {},
   "source": [
    "TokenBlockDataset as IterableDataset\n",
    "\n",
    "This wraps iter_token_blocks and yields tensors of dtype torch.long.\n",
    "It also supports max_blocks control for bounded sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40cc497e-372e-47d2-957c-0284eef87c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a19b2",
   "metadata": {},
   "source": [
    "TokenBlockDataset as IterableDataset\n",
    "\n",
    "This wraps iter_token_blocks and yields tensors of dtype torch.long.\n",
    "It also supports max_blocks control for bounded sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4608203-b074-47f5-9a59-68a28e338205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class TokenBlockDataset(IterableDataset):\n",
    "    def __init__(self, hf_ds, tokenizer, block_size=128, max_blocks=None):\n",
    "        self.hf_ds = hf_ds\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.max_blocks = max_blocks\n",
    "\n",
    "    def __iter__(self):\n",
    "        produced = 0\n",
    "        for block in iter_token_blocks(\n",
    "            self.hf_ds,\n",
    "            self.tokenizer,\n",
    "            block_size=self.block_size,\n",
    "            max_blocks=self.max_blocks or 10**12\n",
    "        ):\n",
    "            yield torch.tensor(block, dtype=torch.long)\n",
    "            produced += 1\n",
    "            if self.max_blocks is not None and produced >= self.max_blocks:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe69029",
   "metadata": {},
   "source": [
    "Collation (pad to same length)\n",
    "\n",
    "Batches are padded to the max seq_len in that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83447b2a-44b7-477b-8fef-a64228224eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_pad(batch):\n",
    "    # batch: List[Tensor(seq_len)]\n",
    "    return torch.nn.utils.rnn.pad_sequence(\n",
    "        batch,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06918bf8",
   "metadata": {},
   "source": [
    "Build DataLoader and sample a few batches\n",
    "\n",
    "Construct dataset with block_size=512, max_blocks=2000, then take first 5 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a371418b-be6f-4449-9e05-2d248a50480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 shape: torch.Size([8, 512])\n",
      "batch 1 shape: torch.Size([8, 512])\n",
      "batch 2 shape: torch.Size([8, 512])\n",
      "batch 3 shape: torch.Size([8, 512])\n",
      "batch 4 shape: torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "dataset_pt = TokenBlockDataset(ds, tokenizer, block_size=512, max_blocks=2000)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset_pt,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_pad,\n",
    "    num_workers=0 \n",
    ")\n",
    "sample_batches = []\n",
    "for i, batch in enumerate(loader):\n",
    "    print(\"batch\", i, \"shape:\", batch.shape)\n",
    "    sample_batches.append(batch)\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bbe67",
   "metadata": {},
   "source": [
    "Save sampled batches to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9d1c2c-a0ba-43a5-a6b0-131ce4aa1f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sample_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "out_path = \"sample_dataset.pt\"\n",
    "torch.save(sample_batches, out_path)\n",
    "print(\"Saved:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
